{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ab784a-1f6a-4b13-b2a4-5f24281a9470",
   "metadata": {},
   "source": [
    "# AP4: Annotation Analysis\n",
    "<b>Sarah Barrington, April 2023</b>\n",
    "\n",
    "This analysis uses the annotated dataset of removed Reddit posts and comments labelled by single thematic categories, which are as follows:\n",
    "\n",
    "* Racial\n",
    "* Gendered\n",
    "* Moderation, censorship, wokeness:\n",
    "* Sexual\n",
    "* Political/geographical\n",
    "* Health\n",
    "* Illegal activities or violence\n",
    "* Criticism:\n",
    "* Environment/world/population\n",
    "* Corporations\n",
    "* Unlabelled\n",
    "\n",
    "The goal of this analysis is to qualify the link between input posts and these resulting categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d20bda-e035-40d6-b4b9-b0551c8b9d76",
   "metadata": {},
   "source": [
    "# Set up and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79f1063b-2245-4188-86c4-239a6b2aa39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory already set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv2tsv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try: \n",
    "    os.chdir('AP4')\n",
    "except:\n",
    "    print('Working directory already set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca467085-516b-407b-b3e6-a0afe9a1edfb",
   "metadata": {},
   "source": [
    "# Divide data into test, training and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5c9c8a4-a6ca-404b-90cc-66489de2680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41667</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>health</td>\n",
       "      <td>So they want to inject shit into people? Liter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135751</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>health</td>\n",
       "      <td>Sort of around the time E-cigs started getting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81008</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>health</td>\n",
       "      <td>A heart attack at 50 cuts off 10-20 years stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24973</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>criticism</td>\n",
       "      <td>Damn, we needed a Harvard University study to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156020</td>\n",
       "      <td>adjudicated</td>\n",
       "      <td>moderation</td>\n",
       "      <td>Great discussion as always on Re- [Removed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0            1           2  \\\n",
       "0   41667  adjudicated      health   \n",
       "1  135751  adjudicated      health   \n",
       "2   81008  adjudicated      health   \n",
       "3   24973  adjudicated   criticism   \n",
       "4  156020  adjudicated  moderation   \n",
       "\n",
       "                                                   3  \n",
       "0  So they want to inject shit into people? Liter...  \n",
       "1  Sort of around the time E-cigs started getting...  \n",
       "2  A heart attack at 50 cuts off 10-20 years stil...  \n",
       "3  Damn, we needed a Harvard University study to ...  \n",
       "4        Great discussion as always on Re- [Removed]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import 'adjudicated' data \n",
    "df = pd.read_csv('adjudicated.txt', sep='\\t', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5380d5b2-e08c-4e9d-8136-09d72b0c2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up data\n",
    "X = df.iloc[:, 3] # Input text\n",
    "y = df.iloc[:, 2] # Annotated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2b72646-ad82-4c3a-a329-4f65464abaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.2\n",
      "0.2\n",
      "550\n"
     ]
    }
   ],
   "source": [
    "# Divide into three groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# Ensure splits have the correct proportions\n",
    "print(len(X_train)/len(df))\n",
    "print(len(X_test)/len(df))\n",
    "print(len(X_dev)/len(df))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b68031ce-905b-4ffa-8036-973d97fa911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt_file(X, y, label):\n",
    "    pd.DataFrame({'text':X, 'label':y}).to_csv(f'splits/{label}.txt', sep=\"\\t\", header=None)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "write_txt_file(X_train, y_train, 'train')\n",
    "write_txt_file(X_test, y_test, 'test')\n",
    "write_txt_file(X_dev, y_dev, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c879049-f7fc-4955-a90c-346cb8a1a9a0",
   "metadata": {},
   "source": [
    "# Build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38c31cad-a0c0-453b-962a-a9a22bc52c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV accuracy: 0.05454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_dev = mlb.transform(y_dev)\n",
    "y_test = mlb.transform(y_test)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_dev = vectorizer.transform(X_dev)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "clf = MultiOutputClassifier(estimator= LogisticRegression()).fit(X_train, y_train)\n",
    "#clf.predict(X[-2:])\n",
    "\n",
    "print('DEV accuracy:', clf.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75e9e110-8820-44cf-8bbc-f0d43af18897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 21)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95074808-6178-44fa-a35a-5d4be0d2ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello/train.txt\n"
     ]
    }
   ],
   "source": [
    "toast = 'hello'\n",
    "print(\"%s/train.txt\" % toast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef7c57-816e-43e2-a6e0-0444f684a14c",
   "metadata": {},
   "source": [
    "# Implementing base BERT model example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cdf1df57-d2f5-42d4-9448-e059f0167b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (4.27.4)\n",
      "Requirement already satisfied: requests in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sarah/.local/share/virtualenvs/259-nlp-dE3sWhq_/lib/python3.9/site-packages (from requests->transformers) (3.0.1)\n",
      "Running on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|███| 213k/213k [00:00<00:00, 23.4MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███| 29.0/29.0 [00:00<00:00, 6.61kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 570/570 [00:00<00:00, 101kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 436M/436M [00:37<00:00, 11.7MB/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, dev accuracy: 0.227\n",
      "Epoch 1, dev accuracy: 0.236\n",
      "Epoch 2, dev accuracy: 0.255\n",
      "Epoch 3, dev accuracy: 0.273\n",
      "Epoch 4, dev accuracy: 0.273\n",
      "Epoch 5, dev accuracy: 0.300\n",
      "Epoch 6, dev accuracy: 0.327\n",
      "Epoch 7, dev accuracy: 0.336\n",
      "Epoch 8, dev accuracy: 0.373\n",
      "Epoch 9, dev accuracy: 0.409\n",
      "Epoch 10, dev accuracy: 0.427\n",
      "Epoch 11, dev accuracy: 0.436\n",
      "Epoch 12, dev accuracy: 0.455\n",
      "Epoch 13, dev accuracy: 0.445\n",
      "Epoch 14, dev accuracy: 0.427\n",
      "Epoch 15, dev accuracy: 0.445\n",
      "Epoch 16, dev accuracy: 0.436\n",
      "Epoch 17, dev accuracy: 0.418\n",
      "Epoch 18, dev accuracy: 0.445\n",
      "No improvement in dev accuracy over 5 epochs; stopping training\n",
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.455\n",
      "Test accuracy for best dev model: 0.482, 95% CIs: [0.388 0.575]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "# If you have your folder of data on your Google drive account, you can connect that here\n",
    "\n",
    "# Change this to the directory with your data\n",
    "directory=\"splits\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on {}\".format(device))\n",
    "def read_labels(filename):\n",
    "    labels={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[2]\n",
    "            if label not in labels:\n",
    "                labels[label]=len(labels)\n",
    "    return labels\n",
    "def read_data(filename, labels, max_data_points=1000):\n",
    "  \n",
    "    data = []\n",
    "    data_labels = []\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[2]\n",
    "            text = cols[1]\n",
    "            \n",
    "            data.append(text)\n",
    "            data_labels.append(labels[label])\n",
    "            \n",
    "\n",
    "    # shuffle the data\n",
    "    tmp = list(zip(data, data_labels))\n",
    "    random.shuffle(tmp)\n",
    "    data, data_labels = zip(*tmp)\n",
    "    \n",
    "    if max_data_points is None:\n",
    "        return data, data_labels\n",
    "    \n",
    "    return data[:max_data_points], data_labels[:max_data_points]\n",
    "\n",
    "labels=read_labels(\"%s/train.txt\" % directory)\n",
    "train_x, train_y=read_data(\"%s/train.txt\" % directory, labels, max_data_points=None)\n",
    "dev_x, dev_y=read_data(\"%s/dev.txt\" % directory, labels, max_data_points=None)\n",
    "test_x, test_y=read_data(\"%s/test.txt\" % directory, labels, max_data_points=None)\n",
    "\n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    corr = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x, y):\n",
    "            y_preds=model.forward(x)\n",
    "            for idx, y_pred in enumerate(y_preds):\n",
    "                prediction=torch.argmax(y_pred)\n",
    "                if prediction == y[idx]:\n",
    "                    corr += 1.\n",
    "                total+=1                          \n",
    "    return corr/total, total\n",
    "class BERTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model_name, params):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.model_name=bert_model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "        \n",
    "        self.num_labels = params[\"label_length\"]\n",
    "\n",
    "        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n",
    "\n",
    "    def get_batches(self, all_x, all_y, batch_size=32, max_toks=510):\n",
    "            \n",
    "        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer \n",
    "      (and limited to a maximum number of WordPiece tokens \"\"\"\n",
    "\n",
    "        batches_x=[]\n",
    "        batches_y=[]\n",
    "        \n",
    "        for i in range(0, len(all_x), batch_size):\n",
    "\n",
    "            current_batch=[]\n",
    "\n",
    "            x=all_x[i:i+batch_size]\n",
    "\n",
    "            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n",
    "            batch_y=all_y[i:i+batch_size]\n",
    "\n",
    "            batches_x.append(batch_x.to(device))\n",
    "            batches_y.append(torch.LongTensor(batch_y).to(device))\n",
    "            \n",
    "        return batches_x, batches_y\n",
    "  \n",
    "\n",
    "    def forward(self, batch_x): \n",
    "    \n",
    "        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n",
    "                         attention_mask=batch_x[\"attention_mask\"],\n",
    "                         token_type_ids=batch_x[\"token_type_ids\"],\n",
    "                         output_hidden_states=True)\n",
    "\n",
    "      # We're going to represent an entire document just by its [CLS] embedding (at position 0)\n",
    "      # And use the *last* layer output (layer -1)\n",
    "      # as a result of this choice, this embedding will be optimized for this purpose during the training process.\n",
    "      \n",
    "        bert_hidden_states = bert_output['hidden_states']\n",
    "\n",
    "        out = bert_hidden_states[-1][:,0,:]\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out.squeeze()\n",
    "def confidence_intervals(accuracy, n, significance_level):\n",
    "    critical_value=(1-significance_level)/2\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
    "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)\n",
    "def train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None):\n",
    "\n",
    "    bert_model = BERTClassifier(bert_model_name, params={\"label_length\": len(labels), \"doLowerCase\":doLowerCase, \"embedding_size\":embedding_size})\n",
    "    bert_model.to(device)\n",
    "\n",
    "    batch_x, batch_y = bert_model.get_batches(train_x, train_y)\n",
    "    dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y)\n",
    "\n",
    "    optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n",
    "    cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs=30\n",
    "    best_dev_acc = 0.\n",
    "    patience=5\n",
    "\n",
    "    best_epoch=0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        bert_model.train()\n",
    "\n",
    "        # Train\n",
    "        for x, y in zip(batch_x, batch_y):\n",
    "            y_pred = bert_model.forward(x)\n",
    "            loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        dev_accuracy, _=evaluate(bert_model, dev_batch_x, dev_batch_y)\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "            if dev_accuracy > best_dev_acc:\n",
    "                torch.save(bert_model.state_dict(), model_filename)\n",
    "                best_dev_acc = dev_accuracy\n",
    "                best_epoch=epoch\n",
    "        if epoch - best_epoch > patience:\n",
    "            print(\"No improvement in dev accuracy over %s epochs; stopping training\" % patience)\n",
    "            break\n",
    "\n",
    "    bert_model.load_state_dict(torch.load(model_filename))\n",
    "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
    "    return bert_model\n",
    "# small BERT -- can run on laptop\n",
    "# bert_model_name=\"google/bert_uncased_L-2_H-128_A-2\"\n",
    "# model_filename=\"mybert.model\"\n",
    "# embedding_size=128\n",
    "# doLowerCase=True\n",
    "\n",
    "# bert-base -- slow on laptop; better on Colab\n",
    "bert_model_name=\"bert-base-cased\"\n",
    "model_filename=\"mybert.model\"\n",
    "embedding_size=768\n",
    "doLowerCase=False\n",
    "\n",
    "model=train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=embedding_size, doLowerCase=doLowerCase)\n",
    "test_batch_x, test_batch_y = model.get_batches(test_x, test_y)\n",
    "accuracy, test_n=evaluate(model, test_batch_x, test_batch_y)\n",
    "\n",
    "lower, upper=confidence_intervals(accuracy, test_n, .95)\n",
    "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7174a4-d707-49c8-a495-a859b4832b5f",
   "metadata": {},
   "source": [
    "# Reporting accuracy and confidence intervals \n",
    "As reported above, the baseline TEST accuracy of the BERT model implemented above is <b>0.482</b>, with lower and upper 95% confidence intervals of <b>[0.388 and 0.575]</b> respectively. This performance is worse than assumed 'chance' rate of 0.5. This suggests that no strong relationship has been found between a BERT model featureset and the human-annotated labels. There is room for improvement on this result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756552b4-ea5c-4cba-8696-a018269a6636",
   "metadata": {},
   "source": [
    "# <font color=red> CATHERINE TO START HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95606a13-dcf6-4aa1-96da-5465898e8f2a",
   "metadata": {},
   "source": [
    "# Analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d66057-3c37-4fa5-af4d-5e455a9deb78",
   "metadata": {},
   "source": [
    "# Tweaking model to improve score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3afff4-db9e-46db-b519-c75ebef80f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "259-nlp",
   "language": "python",
   "name": "259-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
